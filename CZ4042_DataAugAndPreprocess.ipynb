{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bvg6H-848oe8"
      },
      "outputs": [],
      "source": [
        "usingGoogleColab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8fxwlJ1sDSnW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import imgaug as aug\n",
        "import imgaug.augmenters as iaa\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "from statistics import mean\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEIqB_NYGEEl"
      },
      "source": [
        "### Adjustable Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Ut2uiDz4rShs"
      },
      "outputs": [],
      "source": [
        "RandomSeed = 42\n",
        "random.seed(RandomSeed)\n",
        "np.random.seed(RandomSeed)\n",
        "tf.random.set_seed(RandomSeed)\n",
        "aug.seed(RandomSeed)\n",
        "\n",
        "img_width = 32\n",
        "img_height = 32\n",
        "\n",
        "numFold = 10\n",
        "fold = StratifiedKFold(n_splits=numFold, shuffle=True, random_state=RandomSeed)\n",
        "batchSize = 128\n",
        "numEpochs = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ShuffleData(xData, yData):\n",
        "  indices = np.arange(xData.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  return xData[indices], yData[indices]\n",
        "\n",
        "def BuildModel():\n",
        "  # Convolute Layers\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 1)))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "  # Neural Network Layers\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(512, activation='relu'))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  return model\n",
        "\n",
        "def CompileModel(model):\n",
        "  model.compile(optimizer='sgd',\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                metrics=['accuracy'\n",
        "                , tf.keras.metrics.Precision(name='precision')\n",
        "                , tf.keras.metrics.Recall(name='recall')\n",
        "                ])\n",
        "def TrainModel(model, xTrain, yTrain, xTest, yTest):\n",
        "  return model.fit(xTrain, yTrain\n",
        "                   , epochs = numEpochs\n",
        "                   , batch_size= batchSize\n",
        "                   , verbose = 0\n",
        "                   , validation_data=(xTest, yTest))\n",
        "\n",
        "def TrainModel_ClassWeights(model, xTrain, yTrain, xTest, yTest, classWeights):\n",
        "  return model.fit(xTrain, yTrain\n",
        "                   , epochs = numEpochs\n",
        "                   , batch_size = batchSize\n",
        "                   , verbose = 0\n",
        "                   , class_weight = classWeights\n",
        "                   , validation_data=(xTest, yTest))\n",
        "  \n",
        "def GetHistoryAverage(histories):\n",
        "  lastEpochHistory = []\n",
        "\n",
        "  # Take metrics only from last epoch\n",
        "  for foldHist in histories:\n",
        "    myDict = {}\n",
        "    for key in foldHist.history:\n",
        "      myDict[key] = foldHist.history[key][-1]\n",
        "    lastEpochHistory.append(myDict)\n",
        "\n",
        "\n",
        "  print(histories[0].history.keys())\n",
        "\n",
        "  avrgHistory = {}\n",
        "  for key in histories[0].history.keys():\n",
        "    avrgHistory[key] = mean([h[key] for h in lastEpochHistory])\n",
        "    \n",
        "  return avrgHistory\n",
        "\n",
        "def EvaluateModel(model, xData, yData):\n",
        "  metricsResults = model.evaluate(xData, yData\n",
        "                        , verbose = 0\n",
        "                        )\n",
        "  return dict(zip(model.metrics_names, metricsResults))\n",
        "  \n",
        "def GetClassWeights(yData):\n",
        "  uniqueLabels  = np.unique(yData)\n",
        "  classWeightValues = class_weight.compute_class_weight(\n",
        "    'balanced'\n",
        "    , classes = uniqueLabels\n",
        "    , y = yData\n",
        "    )\n",
        "\n",
        "  classWeights = {}\n",
        "  idx = 0\n",
        "  for label in uniqueLabels:\n",
        "    classWeights[label] = classWeightValues[idx]\n",
        "    idx = idx + 1\n",
        "\n",
        "  return classWeights\n",
        "\n",
        "def GenerateAugmentedData(dataset, amountToGen, AugData):\n",
        "  dataLen = len(dataset)\n",
        "  randomIndexList = random.shuffle([*range(dataLen)])\n",
        "  # Augmentation sequence\n",
        "\n",
        "  newData = []\n",
        "  for i in range(amountToGen):\n",
        "    idx = i % dataLen\n",
        "    augmentedImg = AugData.augment(images=dataset[idx])\n",
        "    newData.append(augmentedImg)\n",
        "\n",
        "  return newData"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORfTUsvclX-j"
      },
      "source": [
        "### Loading of Default Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Cw9nekFopo7R"
      },
      "outputs": [],
      "source": [
        "imgs_N_Path = \"Images/Default/NORMAL\"\n",
        "imgs_P_Path = \"Images/Default/PNEUMONIA\"\n",
        "\n",
        "if usingGoogleColab:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  imgs_N_Path = \"drive/MyDrive/CZ4042_Lab02/\" + imgs_N_Path\n",
        "  imgs_P_Path = \"drive/MyDrive/CZ4042_Lab02/\" + imgs_P_Path\n",
        "\n",
        "img_N_List = os.listdir(imgs_N_Path)\n",
        "img_P_List = os.listdir(imgs_P_Path)\n",
        "\n",
        "xN = []\n",
        "xP = []\n",
        "\n",
        "for imgName in img_N_List:\n",
        "  img = load_img(imgs_N_Path + \"/\" + imgName, target_size=(img_width, img_height))\n",
        "  img = img_to_array(img)\n",
        "  grayImg = np.array([[pixel[0] for pixel in row] for row in img]) #Since img is grayscaled, r, g, and b are same values, we only need one of it\n",
        "  xN.append(grayImg)\n",
        "for imgName in img_P_List:\n",
        "  img = load_img(imgs_P_Path + \"/\" + imgName, target_size=(img_width, img_height))\n",
        "  img = img_to_array(img)\n",
        "  grayImg = np.array([[pixel[0] for pixel in row] for row in img])\n",
        "  xP.append(grayImg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TymGpiQ5la9h"
      },
      "source": [
        "### Dataset Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "OxD4Ia9Eq8wW",
        "outputId": "11032c9f-d190-4833-a7ec-b136e4a5357d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1583  Normal Images\n",
            "4273  Pneumonia Images\n",
            "5856  Total Images\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "numNormals = len(xN)\n",
        "numPneumonias = len(xP)\n",
        "\n",
        "print(numNormals, \" Normal Images\")\n",
        "print(numPneumonias, \" Pneumonia Images\")\n",
        "print(numNormals + numPneumonias, \" Total Images\")\n",
        "\n",
        "data = {\n",
        "    'Normal': numNormals\n",
        "    , 'Pneumonia': numPneumonias\n",
        "    }\n",
        "\n",
        "labels = data.keys()\n",
        "values = data.values()\n",
        "plt.bar(labels, values)\n",
        " \n",
        "plt.xlabel(\"Types of Images\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.title(\"Dataset Label Amount Comparison\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "W0xzXpXLDe8y"
      },
      "outputs": [],
      "source": [
        "#Reserve 100 images of each category for testing\n",
        "x = xN + xP\n",
        "y = ([0] * len(xN)) + ([1] * len(xP))\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.1, random_state=RandomSeed)\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "xTest = np.array(xTest, dtype=np.float32)\n",
        "xTrain = np.array(xTrain, dtype=np.float32)\n",
        "yTest = np.array(yTest, dtype=np.float32)\n",
        "yTrain = np.array(yTrain, dtype=np.float32)\n",
        "xTest = xTest / 255.0\n",
        "xTrain = xTrain / 255.0\n",
        "\n",
        "# Shuffle data\n",
        "xTrain, yTrain = ShuffleData(xTrain, yTrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "khwtLkZ5u1ju"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_66 (Conv2D)          (None, 30, 30, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_44 (MaxPoolin  (None, 15, 15, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_67 (Conv2D)          (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_45 (MaxPoolin  (None, 6, 6, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_68 (Conv2D)          (None, 4, 4, 64)          36928     \n",
            "                                                                 \n",
            " flatten_22 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 581,057\n",
            "Trainable params: 581,057\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = BuildModel()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSwUUXffHF93"
      },
      "source": [
        "### Default Scenario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZF1LEkwA-J0",
        "outputId": "858f6f58-7d68-4833-fc63-bf96d1d212c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Done\n"
          ]
        }
      ],
      "source": [
        "xData = xTrain\n",
        "yData = yTrain\n",
        "\n",
        "histories_df = []\n",
        "foldIter = 1\n",
        "for trainIndexes, testIndexes in fold.split(xData, yData):\n",
        "    print(f'\\tTraining {foldIter}/{numFold}: ')\n",
        "    \n",
        "    model = BuildModel()\n",
        "    \n",
        "    CompileModel(model)\n",
        "    \n",
        "    history = TrainModel(model, xData[trainIndexes], yData[trainIndexes], xData[testIndexes], yData[testIndexes])\n",
        "    \n",
        "    histories_df.append(history)\n",
        "    \n",
        "    foldIter += 1\n",
        "\n",
        "clear_output(wait=True)\n",
        "print(\"Training Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LXO6fIH2Alw",
        "outputId": "4904bbca-5c92-456b-87bd-7996e808b2b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'precision', 'recall', 'val_loss', 'val_accuracy', 'val_precision', 'val_recall'])\n",
            "loss            0.4945905417203903\n",
            "accuracy        0.7507484674453735\n",
            "precision       0.748711347579956\n",
            "recall          0.9955434143543244\n",
            "val_loss        0.5340226411819458\n",
            "val_accuracy    0.7366223990917206\n",
            "val_precision   0.7759564876556396\n",
            "val_recall      0.9411917060613633\n"
          ]
        }
      ],
      "source": [
        "avrgHistory_df = GetHistoryAverage(histories_df)\n",
        "\n",
        "for key, value in avrgHistory_df.items():\n",
        "  print(\"{:<15} {:<10}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5619741082191467, 'accuracy': 0.6979522109031677, 'precision': 0.6979522109031677, 'recall': 1.0}\n"
          ]
        }
      ],
      "source": [
        "xDataTrain, xDataTest, yDataTrain, yDataTest = train_test_split(xTrain, yTrain, test_size=0.2, random_state=RandomSeed)\n",
        "\n",
        "model = BuildModel()\n",
        "    \n",
        "CompileModel(model)\n",
        "\n",
        "TrainModel(model, xDataTrain, yDataTrain, xDataTest, yDataTest)\n",
        "\n",
        "metrics_df = EvaluateModel(model, xTest, yTest)\n",
        "\n",
        "print(metrics_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IZoyxvYHLI3"
      },
      "source": [
        "### Adjusting Class Weights Scenario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0.0: 1.8741109530583215, 1.0: 0.681935817805383}\n"
          ]
        }
      ],
      "source": [
        "lw = GetClassWeights(yTrain)\n",
        "\n",
        "print(lw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbjFR-DlDC5v",
        "outputId": "0c7fe90a-617c-46e9-abb1-cd269ceadfd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Done\n"
          ]
        }
      ],
      "source": [
        "xData = xTrain\n",
        "yData = yTrain\n",
        "\n",
        "histories_cw = []\n",
        "foldIter = 1\n",
        "for trainIndexes, testIndexes in fold.split(xData, yData):\n",
        "    print(f'\\tTraining {foldIter}/{numFold}: ')\n",
        "    \n",
        "    model = BuildModel()\n",
        "    \n",
        "    CompileModel(model)\n",
        "\n",
        "    classWeights = GetClassWeights(yData[trainIndexes])\n",
        "    history = TrainModel_ClassWeights(model, xData[trainIndexes], yData[trainIndexes], xData[testIndexes], yData[testIndexes], classWeights)\n",
        "    \n",
        "    histories_cw.append(history)\n",
        "    \n",
        "    foldIter += 1\n",
        "\n",
        "clear_output(wait=True)\n",
        "print(\"Training Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X9Nrn-sCE_J",
        "outputId": "f0432d60-f1d4-4429-e414-d6b2fd375bed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'precision', 'recall', 'val_loss', 'val_accuracy', 'val_precision', 'val_recall'])\n",
            "loss            0.5280583202838898\n",
            "accuracy        0.7913345992565155\n",
            "precision       0.9243667244911193\n",
            "recall          0.7792435884475708\n",
            "val_loss        0.539438220858574\n",
            "val_accuracy    0.740607213973999\n",
            "val_precision   0.8928616583347321\n",
            "val_recall      0.7778654754161834\n"
          ]
        }
      ],
      "source": [
        "avrgHistory_cw = GetHistoryAverage(histories_cw)\n",
        "\n",
        "for key, value in avrgHistory_cw.items():\n",
        "  print(\"{:<15} {:<10}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6554113626480103, 'accuracy': 0.8447098731994629, 'precision': 0.9789156913757324, 'recall': 0.7946210503578186}\n"
          ]
        }
      ],
      "source": [
        "xDataTrain, xDataTest, yDataTrain, yDataTest = train_test_split(xTrain, yTrain, test_size=0.2, random_state=RandomSeed)\n",
        "\n",
        "model = BuildModel()\n",
        "    \n",
        "CompileModel(model)\n",
        "\n",
        "labelWeights = GetClassWeights(yDataTrain)\n",
        "TrainModel_ClassWeights(model, xDataTrain, yDataTrain, xDataTest, yDataTest, labelWeights)\n",
        "\n",
        "metrics_cw = EvaluateModel(model, xTest, yTest)\n",
        "\n",
        "print(metrics_cw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Augmentation Scenario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "z204RF1YXuRN",
        "outputId": "b611ba80-bee3-4c47-d676-3c49edb6f85c"
      },
      "outputs": [],
      "source": [
        "# augData = iaa.OneOf([\n",
        "#       iaa.Fliplr(), # horizontal flips\n",
        "#       iaa.Affine(rotate=5), # roatation\n",
        "#       iaa.Multiply((0.5, 1.5)) # random brightness\n",
        "#       ])\n",
        "\n",
        "# newData = GenerateAugmentedData(x[0:2], 15, augData)\n",
        "\n",
        "# plt.figure(figsize=(50, 20))\n",
        "# for i in range(len(newData)):\n",
        "#   ax = plt.subplot(2, 15, i + 1)\n",
        "#   plt.imshow(newData[i])\n",
        "#   plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ig2wWy8OwvbG"
      },
      "outputs": [],
      "source": [
        "augSequence = iaa.Sequential([\n",
        "      iaa.Affine(rotate=5), # roatation\n",
        "      iaa.Multiply((0.5, 1.5)), # random brightness\n",
        "      iaa.Crop(percent=(0.02, 0.05)) # random crops\n",
        "      ])\n",
        "numNorm = np.count_nonzero(yTrain == 0)\n",
        "numPneum = np.count_nonzero(yTrain == 1)\n",
        "amountToGenerate = numPneum - numNorm\n",
        "newData = GenerateAugmentedData(xTrain, amountToGenerate, augSequence)\n",
        "newData = np.array(newData, dtype=\"float32\")\n",
        "newDataScaled = newData / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Done\n"
          ]
        }
      ],
      "source": [
        "xData = np.append(xTrain, newDataScaled, axis=0)\n",
        "yData = np.append(yTrain, [0] * len(newDataScaled), axis=0)\n",
        "\n",
        "histories_da = []\n",
        "foldIter = 1\n",
        "for trainIndexes, testIndexes in fold.split(xData, yData):\n",
        "    print(f'\\tTraining {foldIter}/{numFold}: ')\n",
        "    \n",
        "    model = BuildModel()\n",
        "    \n",
        "    CompileModel(model)\n",
        "\n",
        "    history = TrainModel(model, xData[trainIndexes], yData[trainIndexes], xData[testIndexes], yData[testIndexes])\n",
        "    \n",
        "    histories_da.append(history)\n",
        "    \n",
        "    foldIter += 1\n",
        "\n",
        "clear_output(wait=True)\n",
        "print(\"Training Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'precision', 'recall', 'val_loss', 'val_accuracy', 'val_precision', 'val_recall'])\n",
            "loss            0.2889170378446579\n",
            "accuracy        0.8711323261260986\n",
            "precision       0.8271640241146088\n",
            "recall          0.938750970363617\n",
            "val_loss        0.30968937277793884\n",
            "val_accuracy    0.8496480941772461\n",
            "val_precision   0.8506993234157563\n",
            "val_recall      0.8826572120189666\n"
          ]
        }
      ],
      "source": [
        "avrgHistory_da = GetHistoryAverage(histories_da)\n",
        "\n",
        "for key, value in avrgHistory_da.items():\n",
        "  print(\"{:<15} {:<10}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4538244605064392, 'accuracy': 0.7901023626327515, 'precision': 0.7698113322257996, 'recall': 0.9975550174713135}\n"
          ]
        }
      ],
      "source": [
        "xDataTrain, xDataTest, yDataTrain, yDataTest = train_test_split(xTrain, yTrain, test_size=0.2, random_state=RandomSeed)\n",
        "\n",
        "newData = GenerateAugmentedData(xDataTrain, amountToGenerate, augSequence)\n",
        "newData = np.array(newData, dtype=\"float32\")\n",
        "newDataScaled = newData / 255.0\n",
        "\n",
        "xDataTrain = np.append(xDataTrain, newDataScaled, axis=0)\n",
        "yDataTrain = np.append(yDataTrain, [False] * len(newDataScaled), axis=0)\n",
        "xDataTrain, yDataTrain = ShuffleData(xDataTrain, yDataTrain)\n",
        "    \n",
        "CompileModel(model)\n",
        "\n",
        "TrainModel(model, xDataTrain, yDataTrain, xDataTest, yDataTest)\n",
        "\n",
        "metrics_da = EvaluateModel(model, xTest, yTest)\n",
        "\n",
        "print(metrics_da)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Augmentation Scenario 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "numNorm = np.count_nonzero(yTrain == 0)\n",
        "amountToGenerate = numNorm // 2\n",
        "newData = GenerateAugmentedData(xTrain, amountToGenerate, augSequence)\n",
        "newData = np.array(newData, dtype=\"float32\")\n",
        "newDataScaled = newData / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Done\n"
          ]
        }
      ],
      "source": [
        "xData = np.append(xTrain, newDataScaled, axis=0)\n",
        "yData = np.append(yTrain, [0] * len(newDataScaled), axis=0)\n",
        "\n",
        "histories_da2 = []\n",
        "foldIter = 1\n",
        "for trainIndexes, testIndexes in fold.split(xData, yData):\n",
        "    print(f'\\tTraining {foldIter}/{numFold}: ')\n",
        "    \n",
        "    model = BuildModel()\n",
        "    \n",
        "    CompileModel(model)\n",
        "\n",
        "    history = TrainModel(model, xData[trainIndexes], yData[trainIndexes], xData[testIndexes], yData[testIndexes])\n",
        "    \n",
        "    histories_da2.append(history)\n",
        "    \n",
        "    foldIter += 1\n",
        "\n",
        "clear_output(wait=True)\n",
        "print(\"Training Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'precision', 'recall', 'val_loss', 'val_accuracy', 'val_precision', 'val_recall'])\n",
            "loss            0.4392691791057587\n",
            "accuracy        0.7927518725395203\n",
            "precision       0.7642959773540496\n",
            "recall          0.9852205991744996\n",
            "val_loss        0.4345978796482086\n",
            "val_accuracy    0.7915494382381439\n",
            "val_precision   0.7688211858272552\n",
            "val_recall      0.9813799440860749\n"
          ]
        }
      ],
      "source": [
        "avrgHistory_da2 = GetHistoryAverage(histories_da2)\n",
        "\n",
        "for key, value in avrgHistory_da2.items():\n",
        "  print(\"{:<15} {:<10}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.23121783137321472, 'accuracy': 0.9095563292503357, 'precision': 0.9299516677856445, 'recall': 0.9413203001022339}\n"
          ]
        }
      ],
      "source": [
        "xDataTrain, xDataTest, yDataTrain, yDataTest = train_test_split(xTrain, yTrain, test_size=0.2, random_state=RandomSeed)\n",
        "\n",
        "newData = GenerateAugmentedData(xDataTrain, amountToGenerate, augSequence)\n",
        "newData = np.array(newData, dtype=\"float32\")\n",
        "newDataScaled = newData / 255.0\n",
        "\n",
        "xDataTrain = np.append(xDataTrain, newDataScaled, axis=0)\n",
        "yDataTrain = np.append(yDataTrain, [False] * len(newDataScaled), axis=0)\n",
        "xDataTrain, yDataTrain = ShuffleData(xDataTrain, yDataTrain)\n",
        "    \n",
        "CompileModel(model)\n",
        "\n",
        "TrainModel(model, xDataTrain, yDataTrain, xDataTest, yDataTest)\n",
        "\n",
        "metrics_da2 = EvaluateModel(model, xTest, yTest)\n",
        "\n",
        "print(metrics_da2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading of Equalized Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "imgs_N_Path = \"Images/GrayscaleEqualized/NORMAL\"\n",
        "imgs_P_Path = \"Images/GrayscaleEqualized/PNEUMONIA\"\n",
        "\n",
        "if usingGoogleColab:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  imgs_N_Path = \"drive/MyDrive/CZ4042_Lab02/\" + imgs_N_Path\n",
        "  imgs_P_Path = \"drive/MyDrive/CZ4042_Lab02/\" + imgs_P_Path\n",
        "\n",
        "img_N_List = os.listdir(imgs_N_Path)\n",
        "img_P_List = os.listdir(imgs_P_Path)\n",
        "\n",
        "xN = []\n",
        "xP = []\n",
        "\n",
        "for imgName in img_N_List:\n",
        "  img = load_img(imgs_N_Path + \"/\" + imgName, target_size=(img_width, img_height))\n",
        "  img = img_to_array(img)\n",
        "  grayImg = np.array([[pixel[0] for pixel in row] for row in img]) #Since img is grayscaled, r, g, and b are same values, we only need one of it\n",
        "  xN.append(grayImg)\n",
        "for imgName in img_P_List:\n",
        "  img = load_img(imgs_P_Path + \"/\" + imgName, target_size=(img_width, img_height))\n",
        "  img = img_to_array(img)\n",
        "  grayImg = np.array([[pixel[0] for pixel in row] for row in img])\n",
        "  xP.append(grayImg)\n",
        "  \n",
        "#Reserve 100 images of each category for testing\n",
        "x = xN + xP\n",
        "y = ([0] * len(xN)) + ([1] * len(xP))\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.1, random_state=RandomSeed)\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "xTest = np.array(xTest, dtype=np.float32)\n",
        "xTrain = np.array(xTrain, dtype=np.float32)\n",
        "yTest = np.array(yTest, dtype=np.float32)\n",
        "yTrain = np.array(yTrain, dtype=np.float32)\n",
        "xTest = xTest / 255.0\n",
        "xTrain = xTrain / 255.0\n",
        "\n",
        "# Shuffle data\n",
        "xTrain, yTrain = ShuffleData(xTrain, yTrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Default Scenario (Equalized Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Done\n"
          ]
        }
      ],
      "source": [
        "xData = xTrain\n",
        "yData = yTrain\n",
        "\n",
        "histories_df2 = []\n",
        "foldIter = 1\n",
        "for trainIndexes, testIndexes in fold.split(xData, yData):\n",
        "    print(f'\\tTraining {foldIter}/{numFold}: ')\n",
        "    \n",
        "    model = BuildModel()\n",
        "    \n",
        "    CompileModel(model)\n",
        "    \n",
        "    history = TrainModel(model, xData[trainIndexes], yData[trainIndexes], xData[testIndexes], yData[testIndexes])\n",
        "    \n",
        "    histories_df2.append(history)\n",
        "    \n",
        "    foldIter += 1\n",
        "\n",
        "clear_output(wait=True)\n",
        "print(\"Training Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'precision', 'recall', 'val_loss', 'val_accuracy', 'val_precision', 'val_recall'])\n",
            "loss            0.3589298754930496\n",
            "accuracy        0.8348091959953308\n",
            "precision       0.854909110069275\n",
            "recall          0.9340350508689881\n",
            "val_loss        0.4284984678030014\n",
            "val_accuracy    0.800379502773285\n",
            "val_precision   0.8403157353401184\n",
            "val_recall      0.9296294093132019\n"
          ]
        }
      ],
      "source": [
        "avrgHistory_df2 = GetHistoryAverage(histories_df2)\n",
        "\n",
        "for key, value in avrgHistory_df2.items():\n",
        "  print(\"{:<15} {:<10}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3428569436073303, 'accuracy': 0.8873720169067383, 'precision': 0.9112709760665894, 'recall': 0.9290953278541565}\n"
          ]
        }
      ],
      "source": [
        "xDataTrain, xDataTest, yDataTrain, yDataTest = train_test_split(xTrain, yTrain, test_size=0.2, random_state=RandomSeed)\n",
        "\n",
        "model = BuildModel()\n",
        "    \n",
        "CompileModel(model)\n",
        "\n",
        "TrainModel(model, xDataTrain, yDataTrain, xDataTest, yDataTest)\n",
        "\n",
        "metrics_df2 = EvaluateModel(model, xTest, yTest)\n",
        "\n",
        "print(metrics_df2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading of Customized Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "imgs_N_Path = \"Images/MyTransform/NORMAL\"\n",
        "imgs_P_Path = \"Images/MyTransform/PNEUMONIA\"\n",
        "\n",
        "if usingGoogleColab:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  imgs_N_Path = \"drive/MyDrive/CZ4042_Lab02/\" + imgs_N_Path\n",
        "  imgs_P_Path = \"drive/MyDrive/CZ4042_Lab02/\" + imgs_P_Path\n",
        "\n",
        "img_N_List = os.listdir(imgs_N_Path)\n",
        "img_P_List = os.listdir(imgs_P_Path)\n",
        "\n",
        "xN = []\n",
        "xP = []\n",
        "\n",
        "for imgName in img_N_List:\n",
        "  img = load_img(imgs_N_Path + \"/\" + imgName, target_size=(img_width, img_height))\n",
        "  img = img_to_array(img)\n",
        "  grayImg = np.array([[pixel[0] for pixel in row] for row in img]) #Since img is grayscaled, r, g, and b are same values, we only need one of it\n",
        "  xN.append(grayImg)\n",
        "for imgName in img_P_List:\n",
        "  img = load_img(imgs_P_Path + \"/\" + imgName, target_size=(img_width, img_height))\n",
        "  img = img_to_array(img)\n",
        "  grayImg = np.array([[pixel[0] for pixel in row] for row in img])\n",
        "  xP.append(grayImg)\n",
        "  \n",
        "#Reserve 100 images of each category for testing\n",
        "x = xN + xP\n",
        "y = ([0] * len(xN)) + ([1] * len(xP))\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.1, random_state=RandomSeed)\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "xTest = np.array(xTest, dtype=np.float32)\n",
        "xTrain = np.array(xTrain, dtype=np.float32)\n",
        "yTest = np.array(yTest, dtype=np.float32)\n",
        "yTrain = np.array(yTrain, dtype=np.float32)\n",
        "xTest = xTest / 255.0\n",
        "xTrain = xTrain / 255.0\n",
        "\n",
        "# Shuffle data\n",
        "xTrain, yTrain = ShuffleData(xTrain, yTrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Default Scenario (Custom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Done\n"
          ]
        }
      ],
      "source": [
        "xData = xTrain\n",
        "yData = yTrain\n",
        "\n",
        "histories_df3 = []\n",
        "foldIter = 1\n",
        "for trainIndexes, testIndexes in fold.split(xData, yData):\n",
        "    print(f'\\tTraining {foldIter}/{numFold}: ')\n",
        "    \n",
        "    model = BuildModel()\n",
        "    \n",
        "    CompileModel(model)\n",
        "    \n",
        "    history = TrainModel(model, xData[trainIndexes], yData[trainIndexes], xData[testIndexes], yData[testIndexes])\n",
        "    \n",
        "    histories_df3.append(history)\n",
        "    \n",
        "    foldIter += 1\n",
        "\n",
        "clear_output(wait=True)\n",
        "print(\"Training Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'precision', 'recall', 'val_loss', 'val_accuracy', 'val_precision', 'val_recall'])\n",
            "loss            0.327982971072197\n",
            "accuracy        0.8596879541873932\n",
            "precision       0.881434565782547\n",
            "recall          0.9344376385211944\n",
            "val_loss        0.38834020793437957\n",
            "val_accuracy    0.8278937339782715\n",
            "val_precision   0.8672353565692902\n",
            "val_recall      0.9223761856555939\n"
          ]
        }
      ],
      "source": [
        "avrgHistory_df3 = GetHistoryAverage(histories_df3)\n",
        "\n",
        "for key, value in avrgHistory_df3.items():\n",
        "  print(\"{:<15} {:<10}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.30013179779052734, 'accuracy': 0.8754266500473022, 'precision': 0.9137930870056152, 'recall': 0.9070904850959778}\n"
          ]
        }
      ],
      "source": [
        "xDataTrain, xDataTest, yDataTrain, yDataTest = train_test_split(xTrain, yTrain, test_size=0.2, random_state=RandomSeed)\n",
        "\n",
        "model = BuildModel()\n",
        "    \n",
        "CompileModel(model)\n",
        "\n",
        "TrainModel(model, xDataTrain, yDataTrain, xDataTest, yDataTest)\n",
        "\n",
        "metrics_df3 = EvaluateModel(model, xTest, yTest)\n",
        "\n",
        "print(metrics_df3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             scenario      loss  accuracy  precision    recall\n",
            "0       Default Train  0.494591  0.750748   0.748711  0.995543\n",
            "1      Default2 Train  0.358930  0.834809   0.854909  0.934035\n",
            "2      Default3 Train  0.327983  0.859688   0.881435  0.934438\n",
            "3  ClassWeights Train  0.528058  0.791335   0.924367  0.779244\n",
            "4       DataAug Train  0.288917  0.871132   0.827164  0.938751\n",
            "5      DataAug2 Train  0.439269  0.792752   0.764296  0.985221\n",
            "\n",
            "                 scenario      loss  accuracy  precision    recall\n",
            "6        Default Validate  0.534023  0.736622   0.775956  0.941192\n",
            "7       Default2 Validate  0.428498  0.800380   0.840316  0.929629\n",
            "8       Default3 Validate  0.388340  0.827894   0.867235  0.922376\n",
            "9   ClassWeights Validate  0.539438  0.740607   0.892862  0.777865\n",
            "10       DataAug Validate  0.309689  0.849648   0.850699  0.882657\n",
            "11      DataAug2 Validate  0.434598  0.791549   0.768821  0.981380\n",
            "\n",
            "             scenario      loss  accuracy  precision    recall\n",
            "12       Default Test  0.561974  0.697952   0.697952  1.000000\n",
            "13      Default2 Test  0.342857  0.887372   0.911271  0.929095\n",
            "14      Default3 Test  0.300132  0.875427   0.913793  0.907090\n",
            "15  ClassWeights Test  0.655411  0.844710   0.978916  0.794621\n",
            "16       DataAug Test  0.453824  0.790102   0.769811  0.997555\n",
            "17      DataAug2 Test  0.231218  0.909556   0.929952  0.941320\n"
          ]
        }
      ],
      "source": [
        "metricsNames = ['loss', 'accuracy', 'precision', 'recall']\n",
        "valMetricsNames = ['val_loss', 'val_accuracy', 'val_precision', 'val_recall']\n",
        "\n",
        "df = pd.DataFrame(columns=['scenario'] + metricsNames)\n",
        "\n",
        "df.loc[len(df.index)] = [\"Default Train\"] + [avrgHistory_df[name] for name in metricsNames]\n",
        "df.loc[len(df.index)] = [\"Default2 Train\"] + [avrgHistory_df2[name] for name in metricsNames]\n",
        "df.loc[len(df.index)] = [\"Default3 Train\"] + [avrgHistory_df3[name] for name in metricsNames]\n",
        "df.loc[len(df.index)] = [\"ClassWeights Train\"] + [avrgHistory_cw[name] for name in metricsNames]\n",
        "df.loc[len(df.index)] = [\"DataAug Train\"] + [avrgHistory_da[name] for name in metricsNames]\n",
        "df.loc[len(df.index)] = [\"DataAug2 Train\"] + [avrgHistory_da2[name] for name in metricsNames]\n",
        "\n",
        "df.loc[len(df.index)] = [\"Default Validate\"] + [avrgHistory_df[name] for name in valMetricsNames]\n",
        "df.loc[len(df.index)] = [\"Default2 Validate\"] + [avrgHistory_df2[name] for name in valMetricsNames]\n",
        "df.loc[len(df.index)] = [\"Default3 Validate\"] + [avrgHistory_df3[name] for name in valMetricsNames]\n",
        "df.loc[len(df.index)] = [\"ClassWeights Validate\"] + [avrgHistory_cw[name] for name in valMetricsNames]\n",
        "df.loc[len(df.index)] = [\"DataAug Validate\"] + [avrgHistory_da[name] for name in valMetricsNames]\n",
        "df.loc[len(df.index)] = [\"DataAug2 Validate\"] + [avrgHistory_da2[name] for name in valMetricsNames]\n",
        "\n",
        "df.loc[len(df.index)] = [\"Default Test\"] + list(metrics_df.values())\n",
        "df.loc[len(df.index)] = [\"Default2 Test\"] + list(metrics_df2.values())\n",
        "df.loc[len(df.index)] = [\"Default3 Test\"] + list(metrics_df3.values())\n",
        "df.loc[len(df.index)] = [\"ClassWeights Test\"] + list(metrics_cw.values())\n",
        "df.loc[len(df.index)] = [\"DataAug Test\"] + list(metrics_da.values())\n",
        "df.loc[len(df.index)] = [\"DataAug2 Test\"] + list(metrics_da2.values())\n",
        "\n",
        "\n",
        "print(df.loc[:5])\n",
        "print()\n",
        "print(df.loc[6:11])\n",
        "print()\n",
        "print(df.loc[12:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "65463cf6ea576c082948f46708866b2d2d923560ace25e7a44cd8769167a37ab"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
